{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:03:33.971405Z","iopub.execute_input":"2025-08-10T05:03:33.971644Z","iopub.status.idle":"2025-08-10T05:03:34.377394Z","shell.execute_reply.started":"2025-08-10T05:03:33.971620Z","shell.execute_reply":"2025-08-10T05:03:34.376754Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip -q install \"transformers>=4.42.0\" \"datasets>=2.19.0\" accelerate scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:03:37.427388Z","iopub.execute_input":"2025-08-10T05:03:37.427670Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os, random, numpy as np, torch\nfrom datasets import load_dataset, DatasetDict, Dataset, concatenate_datasets\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n                          DataCollatorWithPadding, Trainer, TrainingArguments, set_seed)\n\n# Config\nSEED = 42; set_seed(SEED)\nTEXT, LABEL = \"text\", \"label\"\nNUM_LABELS = 2\nMAX_LEN = 192\nSUBSET_SIZE = 1000\nEPOCHS_SUB = 2\nEPOCHS_FULL = 5\nBS = 16\nLR = 2e-5\n\nMODEL_IDS = {\n    \"bert\"     : \"bert-base-uncased\",\n    \"roberta\"  : \"roberta-base\",\n    \"deberta\"  : \"microsoft/deberta-v3-base\",\n    \"electra\"  : \"google/electra-base-discriminator\",\n    \"distilbert\": \"distilbert-base-uncased\",\n}\n\nWK = \"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else \".\"\nOUT = os.path.join(WK, \"runs\"); BEST_DIR = os.path.join(WK, \"best\")\nos.makedirs(OUT, exist_ok=True); os.makedirs(BEST_DIR, exist_ok=True)\n\n# Metrics\ndef f1_macro(y_true, y_pred): \n    return f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = logits.argmax(axis=-1)\n    return {\"accuracy\": float(accuracy_score(labels, preds)),\n            \"f1_macro\": float(f1_macro(labels, preds))}\n\n# Data loader\ndef load_imdb() -> DatasetDict:\n    kaggle_csv = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n    try:\n        ds = load_dataset(\"imdb\")\n        split = ds[\"train\"].train_test_split(test_size=0.1, seed=SEED, stratify_by_column=\"label\")\n        return DatasetDict(train=split[\"train\"], validation=split[\"test\"], test=ds[\"test\"])\n    except Exception:\n        import pandas as pd\n        if not os.path.exists(kaggle_csv):\n            raise FileNotFoundError(\"Add 'IMDB Dataset of 50K Movie Reviews' to Inputs or enable internet.\")\n        df = pd.read_csv(kaggle_csv)\n        df[\"label\"] = (df[\"sentiment\"].str.lower()==\"positive\").astype(int)\n        df.rename(columns={\"review\":\"text\"}, inplace=True)\n        df = df[[TEXT, LABEL]]\n        from sklearn.model_selection import train_test_split\n        tr, tmp = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[LABEL])\n        va, te = train_test_split(tmp, test_size=0.5, random_state=SEED, stratify=tmp[LABEL])\n        return DatasetDict(\n            train=Dataset.from_pandas(tr.reset_index(drop=True)),\n            validation=Dataset.from_pandas(va.reset_index(drop=True)),\n            test=Dataset.from_pandas(te.reset_index(drop=True)),\n        )\n\n# Subset\ndef stratified_subset(dataset, n, label_col=LABEL):\n    if n >= len(dataset): return dataset\n    labels = dataset[label_col]; classes = sorted(set(labels))\n    per = max(1, n // len(classes))\n    idx_by_c = {c: [] for c in classes}\n    for i, lab in enumerate(labels): idx_by_c[lab].append(i)\n    rng = np.random.default_rng(SEED); chosen = []\n    for c in classes:\n        ii = idx_by_c[c]; rng.shuffle(ii); chosen += ii[:per]\n    rem = n - len(chosen)\n    if rem > 0:\n        rest = list(set(range(len(dataset))) - set(chosen)); rng.shuffle(rest); chosen += rest[:rem]\n    return dataset.select(sorted(chosen))\n\ndef tokenize(batch, tok): \n    return tok(batch[TEXT], truncation=True, max_length=MAX_LEN)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:12:24.643953Z","iopub.execute_input":"2025-08-10T05:12:24.644259Z","iopub.status.idle":"2025-08-10T05:12:24.658739Z","shell.execute_reply.started":"2025-08-10T05:12:24.644238Z","shell.execute_reply":"2025-08-10T05:12:24.658217Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"raw = load_imdb()\nsmall_train = stratified_subset(raw[\"train\"], SUBSET_SIZE)\nds_small = DatasetDict(train=small_train, validation=raw[\"validation\"])\nprint(\"Train small:\", len(ds_small[\"train\"]), \"| Val:\", len(ds_small[\"validation\"]), \"| Test:\", len(raw[\"test\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:12:26.993647Z","iopub.execute_input":"2025-08-10T05:12:26.993904Z","iopub.status.idle":"2025-08-10T05:12:29.012034Z","shell.execute_reply.started":"2025-08-10T05:12:26.993883Z","shell.execute_reply":"2025-08-10T05:12:29.011318Z"}},"outputs":[{"name":"stdout","text":"Train small: 1000 | Val: 2500 | Test: 25000\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def train_on_subset(model_id: str):\n    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    tok_ds = ds_small.map(lambda x: tokenize(x, tok), batched=True, remove_columns=[TEXT])\n    coll = DataCollatorWithPadding(tokenizer=tok)\n    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=NUM_LABELS)\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    args = TrainingArguments(\n        output_dir=os.path.join(OUT, model_id.replace(\"/\", \"_\")+\"_subset\"),\n        per_device_train_batch_size=BS,\n        per_device_eval_batch_size=BS,\n        num_train_epochs=EPOCHS_SUB,\n        learning_rate=LR,\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\",\n        seed=SEED,\n        logging_steps=50,\n        save_strategy=\"no\",\n    )\n    tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"], \n                 eval_dataset=tok_ds[\"validation\"], tokenizer=tok, \n                 data_collator=coll, compute_metrics=compute_metrics)\n    tr.train()\n    m = tr.evaluate(tok_ds[\"validation\"])\n    return {\"id\": model_id, \"f1_macro\": m[\"eval_f1_macro\"]}\n\ndef train_full(model_id: str):\n    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    tok_ds = raw.map(lambda x: tokenize(x, tok), batched=True, remove_columns=[TEXT])\n    coll = DataCollatorWithPadding(tokenizer=tok)\n    model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=NUM_LABELS)\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    merged = concatenate_datasets([tok_ds[\"train\"], tok_ds[\"validation\"]])\n    args = TrainingArguments(\n        output_dir=BEST_DIR,\n        per_device_train_batch_size=BS,\n        per_device_eval_batch_size=BS,\n        num_train_epochs=EPOCHS_FULL,\n        learning_rate=LR,\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\",\n        seed=SEED,\n        logging_steps=50,\n        save_strategy=\"no\",\n    )\n    tr = Trainer(model=model, args=args, train_dataset=merged, \n                 eval_dataset=tok_ds[\"test\"], tokenizer=tok, \n                 data_collator=coll, compute_metrics=compute_metrics)\n    tr.train()\n    test_metrics = tr.evaluate(tok_ds[\"test\"])\n    tr.save_model(BEST_DIR); tok.save_pretrained(BEST_DIR)\n    return {\"tok\": tok, \"test\": test_metrics}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:12:31.553938Z","iopub.execute_input":"2025-08-10T05:12:31.554243Z","iopub.status.idle":"2025-08-10T05:12:31.562893Z","shell.execute_reply.started":"2025-08-10T05:12:31.554217Z","shell.execute_reply":"2025-08-10T05:12:31.562143Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"res_bert = train_on_subset(MODEL_IDS[\"bert\"])\nprint(\"BERT:\", res_bert)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:12:34.962615Z","iopub.execute_input":"2025-08-10T05:12:34.963196Z","iopub.status.idle":"2025-08-10T05:13:38.875630Z","shell.execute_reply.started":"2025-08-10T05:12:34.963159Z","shell.execute_reply":"2025-08-10T05:13:38.874978Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c7a80dfa74406eba28916e778d24e3"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4208541600.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"],\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 00:41, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.544800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"BERT: {'id': 'bert-base-uncased', 'f1_macro': 0.8495074410565475}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"res_roberta = train_on_subset(MODEL_IDS[\"roberta\"])\nprint(\"RoBERTa:\", res_roberta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:14:44.103451Z","iopub.execute_input":"2025-08-10T05:14:44.104086Z","iopub.status.idle":"2025-08-10T05:15:50.209678Z","shell.execute_reply.started":"2025-08-10T05:14:44.104062Z","shell.execute_reply":"2025-08-10T05:15:50.208867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc369ec6ad64c0b9cdb98ddce655006"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4208541600.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"],\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 00:44, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.605000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:18]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"RoBERTa: {'id': 'roberta-base', 'f1_macro': 0.8935975484875172}\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"res_deberta = train_on_subset(MODEL_IDS[\"deberta\"])\nprint(\"DeBERTa:\", res_deberta)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:15:55.612673Z","iopub.execute_input":"2025-08-10T05:15:55.613404Z","iopub.status.idle":"2025-08-10T05:17:32.500233Z","shell.execute_reply.started":"2025-08-10T05:15:55.613379Z","shell.execute_reply":"2025-08-10T05:17:32.499512Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6da1e76983e347989dbe5882bcf3e810"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd6b578ec4734b3eace7c3588b252aaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18cfff598a7f483c957c49dbce7455b8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82a29aca4854c10b104686d05d11dde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53a034b081354c9caf5d3783d969ab9c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4023471fae8848e9ab1e580d8a341224"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f596737f4e5741a1a9d49cfc2c936202"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4208541600.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"],\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 00:58, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.495900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:25]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"DeBERTa: {'id': 'microsoft/deberta-v3-base', 'f1_macro': 0.9115937620558506}\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Drop this in a new cell BEFORE training ModernBERT\nimport os, torch\nos.environ[\"TORCHDYNAMO_DISABLE\"] = \"1\"   # fully disable torch.compile/dynamo\n\nfrom transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, Trainer, TrainingArguments\n\ndef train_on_subset_modernbert(model_id):\n    # clone of your train_on_subset but with compile turned off in config\n    cfg = AutoConfig.from_pretrained(model_id)\n    for attr in (\"reference_compile\", \"torch_compile\", \"use_compiled_kernels\"):\n        if hasattr(cfg, attr):\n            setattr(cfg, attr, False)\n\n    tok = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n    tok_ds = ds_small.map(lambda x: tokenize(x, tok), batched=True, remove_columns=[TEXT])\n    coll = DataCollatorWithPadding(tokenizer=tok)\n\n    model = AutoModelForSequenceClassification.from_pretrained(model_id, config=cfg, num_labels=NUM_LABELS)\n    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    args = TrainingArguments(\n        output_dir=os.path.join(OUT, model_id.replace(\"/\", \"_\")+\"_subset\"),\n        per_device_train_batch_size=BS,\n        per_device_eval_batch_size=BS,\n        num_train_epochs=EPOCHS_SUB,\n        learning_rate=LR,\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\",\n        seed=SEED,\n        logging_steps=50,\n        save_strategy=\"no\",\n    )\n    tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"],\n                 eval_dataset=tok_ds[\"validation\"], tokenizer=tok,\n                 data_collator=coll, compute_metrics=compute_metrics)\n    tr.train()\n    m = tr.evaluate(tok_ds[\"validation\"])\n    return {\"id\": model_id, \"f1_macro\": m[\"eval_f1_macro\"]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T04:59:22.364695Z","iopub.execute_input":"2025-08-10T04:59:22.365361Z","iopub.status.idle":"2025-08-10T04:59:22.373050Z","shell.execute_reply.started":"2025-08-10T04:59:22.365338Z","shell.execute_reply":"2025-08-10T04:59:22.372246Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"res_electra = train_on_subset(MODEL_IDS[\"electra\"])\nprint(\"Electra:\", res_electra)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:17:55.712516Z","iopub.execute_input":"2025-08-10T05:17:55.712752Z","iopub.status.idle":"2025-08-10T05:19:08.029270Z","shell.execute_reply.started":"2025-08-10T05:17:55.712737Z","shell.execute_reply":"2025-08-10T05:19:08.028561Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"236b9f14968c40e88231024281816475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85787c68f1fb4e90985866fce9097cfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3a8b644a351446a912d0ce903daacca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f741cbf790442298772e162a6b39e02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0683c1b0c059403c836e2757cc0ec480"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0575855627f14a8fa5233ef0b109dd73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb009bbce344874b93a75f9c71adc8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eab7ef2c09e47279d2c4e6711279f8a"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4208541600.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"],\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 00:44, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.606300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:19]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Electra: {'id': 'google/electra-base-discriminator', 'f1_macro': 0.8838926982760388}\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"res_distilbert = train_on_subset(MODEL_IDS[\"distilbert\"])\nprint(\"DistilBERT:\", res_distilbert)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:19:19.392648Z","iopub.execute_input":"2025-08-10T05:19:19.392920Z","iopub.status.idle":"2025-08-10T05:19:55.503924Z","shell.execute_reply.started":"2025-08-10T05:19:19.392901Z","shell.execute_reply":"2025-08-10T05:19:55.503169Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4096d367b4bb4bd7919329d02410c4ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d795bfc5d7c240f4bf08bb18e111e169"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e38ad4a427f479097b6e15313fa5d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff4e18ee8874f1eb7f03eaec4463fd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3e3d3bd398a48569f61c48bcdf8aeb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7701b79458043849d8f5dca251fed88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b27d699d254b0597ba25f009797acc"}},"metadata":{}},{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4208541600.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  tr = Trainer(model=model, args=args, train_dataset=tok_ds[\"train\"],\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='64' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [64/64 00:20, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.638300</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='79' max='79' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [79/79 00:09]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"DistilBERT: {'id': 'distilbert-base-uncased', 'f1_macro': 0.8367895545314901}\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# Collect only the results you actually ran\nall_res = []\nfor r in [globals().get(\"res_bert\"),\n          globals().get(\"res_roberta\"),\n          globals().get(\"res_deberta\"),\n          globals().get(\"res_electra\"),\n          globals().get(\"res_distilbert\")]:\n    if isinstance(r, dict) and \"f1_macro\" in r:\n        all_res.append(r)\n\n# If you only ran DistilBERT, this will just pick that.\nbest = max(all_res, key=lambda d: d[\"f1_macro\"])\nbest_id = best[\"id\"]\nprint(\"Best on subset:\", best_id, \"| f1_macro:\", best[\"f1_macro\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:21:00.382927Z","iopub.execute_input":"2025-08-10T05:21:00.383535Z","iopub.status.idle":"2025-08-10T05:21:00.388839Z","shell.execute_reply.started":"2025-08-10T05:21:00.383512Z","shell.execute_reply":"2025-08-10T05:21:00.388158Z"}},"outputs":[{"name":"stdout","text":"Best on subset: microsoft/deberta-v3-base | f1_macro: 0.9115937620558506\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from transformers import pipeline, AutoTokenizer\n\n# assumes you've already run the \"pick best\" cell and have best_id\nprint(\"Sanity-checking pipeline on:\", best_id)\n\nclf_pipeline = pipeline(\n    task=\"text-classification\",\n    model=best_id,                 # if you didn't save subset fine-tune, this loads base weights\n    tokenizer=best_id,\n    device=0 if torch.cuda.is_available() else -1,\n    truncation=True,\n    return_all_scores=True,\n)\n\n# Try a few validation samples just to confirm everything runs end-to-end\nsample_idxs = list(range(len(ds_small[\"validation\"])))[:5]\ntexts  = [ds_small[\"validation\"][i][TEXT]  for i in sample_idxs]\n\npreds = clf_pipeline(texts, batch_size=16)\nfor i, (t, pr) in enumerate(zip(texts, preds), 1):\n    scores = {d[\"label\"]: d[\"score\"] for d in pr}\n    p_neg = float(scores.get(\"LABEL_0\", scores.get(\"NEGATIVE\", 0.0)))\n    p_pos = float(scores.get(\"LABEL_1\", scores.get(\"POSITIVE\", 0.0)))\n    pred  = \"positive\" if p_pos >= p_neg else \"negative\"\n    conf  = max(p_neg, p_pos)\n    snippet = t if len(t) < 180 else t[:177] + \"...\"\n    print(f\"\\n[{i}] {snippet}\\nPred: {pred} | Conf: {conf:.3f} | P(neg)={p_neg:.3f}, P(pos)={p_pos:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:24:58.263074Z","iopub.execute_input":"2025-08-10T05:24:58.263920Z","iopub.status.idle":"2025-08-10T05:25:02.170763Z","shell.execute_reply.started":"2025-08-10T05:24:58.263884Z","shell.execute_reply":"2025-08-10T05:25:02.169987Z"}},"outputs":[{"name":"stdout","text":"Sanity-checking pipeline on: microsoft/deberta-v3-base\n","output_type":"stream"},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nDevice set to use cuda:0\n/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\nAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","output_type":"stream"},{"name":"stdout","text":"\n[1] I have watched 3 episodes of Caveman, and I have no idea why I continue except maybe waiting for it to get better. <br /><br />To me this show is just pumping itself off the com...\nPred: negative | Conf: 0.517 | P(neg)=0.517, P(pos)=0.483\n\n[2] The fluttering of butterfly wings in the Atlantic can unleash a hurricane in the Pacific. According to this theory (somehow related to the Chaos Theory, I'm not sure exactly how...\nPred: negative | Conf: 0.515 | P(neg)=0.515, P(pos)=0.485\n\n[3] Judging from this film and THE STRONG MAN, made the same year, I would not place Harry Langdon at the top of the list of great silent screen comedians. There simply is not enoug...\nPred: negative | Conf: 0.517 | P(neg)=0.517, P(pos)=0.483\n\n[4] Earlier today I got into an argument on why so many people complain about modern films in which I encountered a curious statement: \"the character development in newer movies jus...\nPred: negative | Conf: 0.520 | P(neg)=0.520, P(pos)=0.480\n\n[5] this movie is such a moving, amazing piece of work. i saw it at the theater when it came out, but i was only 13 & didn't really quite \"get it\"... i saw it again when i was 20 (o...\nPred: negative | Conf: 0.515 | P(neg)=0.515, P(pos)=0.485\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"bundle = train_full(best_id)\nprint(\"Final TEST metrics:\", bundle[\"test\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T05:25:22.294325Z","iopub.execute_input":"2025-08-10T05:25:22.294589Z","execution_failed":"2025-08-10T05:27:11.165Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58d7edf39bdb408ca451da3e784eba3b"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4208541600.py:47: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  tr = Trainer(model=model, args=args, train_dataset=merged,\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='112' max='3910' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 112/3910 01:42 < 58:57, 1.07 it/s, Epoch 0.14/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.545200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.280900</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Use the just-saved model + tokenizer directory\nMODEL_PATH = BEST_DIR\n\nclf_pipeline = pipeline(\n    task=\"text-classification\",\n    model=MODEL_PATH,\n    tokenizer=MODEL_PATH,\n    device=0 if torch.cuda.is_available() else -1,\n    truncation=True,\n    return_all_scores=True,\n)\nclf_pipeline\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-10T05:27:11.166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random, numpy as np\n\n# sample 10 texts from the test split you loaded earlier\nidxs = list(range(len(raw[\"test\"])))\nrandom.Random(SEED).shuffle(idxs)\nidxs = idxs[:10]\n\ntexts  = [raw[\"test\"][i][TEXT]  for i in idxs]\nlabels = [raw[\"test\"][i][LABEL] for i in idxs]  # 0=neg, 1=pos\n\n# batch inference via pipeline\npreds = clf_pipeline(texts, batch_size=32)\n\nfor i, (t, pr, y) in enumerate(zip(texts, preds, labels), 1):\n    # pr is a list like [{'label': 'LABEL_0', 'score': ...}, {'label': 'LABEL_1', 'score': ...}]\n    scores = {d[\"label\"]: d[\"score\"] for d in pr}\n    p_neg = float(scores.get(\"LABEL_0\", scores.get(\"NEGATIVE\", 0.0)))\n    p_pos = float(scores.get(\"LABEL_1\", scores.get(\"POSITIVE\", 0.0)))\n    pred  = 1 if p_pos >= p_neg else 0\n    conf  = max(p_neg, p_pos)\n\n    snippet = t if len(t) < 280 else t[:277] + \"...\"\n    print(f\"\\n[{i}] {snippet}\")\n    print(f\"Pred: {'positive' if pred==1 else 'negative'} | True: {'positive' if y==1 else 'negative'}\")\n    print(f\"Confidence: {conf:.3f} | P(neg)={p_neg:.3f}, P(pos)={p_pos:.3f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-10T05:27:11.166Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}