{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":320111,"datasetId":134715,"databundleVersionId":333307,"isSourceIdPinned":false}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n\nprint(\"Path to dataset files:\", path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:30:55.946625Z","iopub.execute_input":"2025-09-05T19:30:55.946863Z","iopub.status.idle":"2025-09-05T19:30:58.026451Z","shell.execute_reply.started":"2025-09-05T19:30:55.946843Z","shell.execute_reply":"2025-09-05T19:30:58.025060Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# !pip install pandas scikit-learn torch torchvision torchaudio tqdm kagglehub --quiet\n\nimport os, re, random, gc, glob\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm\n\n# ---- Kaggle dataset path ----\nimport kagglehub\nDATASET_DIR = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\nprint(\"DATASET_DIR:\", DATASET_DIR)\n\n# Try to auto-detect the CSV\ncandidates = glob.glob(os.path.join(DATASET_DIR, \"**\", \"*.csv\"), recursive=True)\nCSV_FILE = os.path.basename(candidates[0]) if candidates else \"IMDB Dataset.csv\"\nprint(\"CSV_FILE:\", CSV_FILE)\n\n# ---- General config ----\nTEXT_COL, LABEL_COL = \"review\", \"sentiment\"\nMIN_FREQ, MAX_VOCAB_SIZE = 2, 50000\nMAX_LEN = 250\n\nEMBED_DIM = 100          # use 100 if you load glove.6B.100d.txt\nHIDDEN_DIM = 128\nBATCH_SIZE = 64\nEPOCHS, LR = 5, 2e-3\nVAL_SPLIT, TEST_SPLIT = 0.1, 0.1\nSEED = 42\n\n# GloVe (set to file path or keep None for learned embeddings)\nGLOVE_TXT_PATH = None  # e.g., \"/content/glove.6B.100d.txt\"\n\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:16.149294Z","iopub.execute_input":"2025-09-05T19:37:16.149540Z","iopub.status.idle":"2025-09-05T19:37:23.343878Z","shell.execute_reply.started":"2025-09-05T19:37:16.149522Z","shell.execute_reply":"2025-09-05T19:37:23.343152Z"}},"outputs":[{"name":"stdout","text":"DATASET_DIR: /kaggle/input/imdb-dataset-of-50k-movie-reviews\nCSV_FILE: IMDB Dataset.csv\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"def basic_clean(text: str) -> str:\n    text = text.lower()\n    text = re.sub(r\"<br\\s*/?>\", \" \", text)\n    text = re.sub(r\"<.*?>\", \" \", text)\n    text = re.sub(r\"[^a-z0-9' ]+\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndf = pd.read_csv(os.path.join(DATASET_DIR, CSV_FILE))\ndf = df[[TEXT_COL, LABEL_COL]].dropna()\n\nlabel_map = {\"negative\": 0, \"positive\": 1}\ndf[\"label\"] = df[LABEL_COL].map(label_map).astype(int)\ndf[\"clean\"] = df[TEXT_COL].map(basic_clean)\n\ndf.head(3)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:26.173305Z","iopub.execute_input":"2025-09-05T19:37:26.173626Z","iopub.status.idle":"2025-09-05T19:37:33.208157Z","shell.execute_reply.started":"2025-09-05T19:37:26.173598Z","shell.execute_reply":"2025-09-05T19:37:33.207227Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                              review sentiment  label  \\\n0  One of the other reviewers has mentioned that ...  positive      1   \n1  A wonderful little production. <br /><br />The...  positive      1   \n2  I thought this was a wonderful way to spend ti...  positive      1   \n\n                                               clean  \n0  one of the other reviewers has mentioned that ...  \n1  a wonderful little production the filming tech...  \n2  i thought this was a wonderful way to spend ti...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n      <th>label</th>\n      <th>clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>one of the other reviewers has mentioned that ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>a wonderful little production the filming tech...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n      <td>1</td>\n      <td>i thought this was a wonderful way to spend ti...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def tokenize(s: str):\n    return s.split()\n\ncounter = Counter()\nfor s in df[\"clean\"]:\n    counter.update(tokenize(s))\n\nPAD, UNK, BOS, EOS = \"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"\n\nmost_common = [w for w, c in counter.most_common() if c >= MIN_FREQ]\nmost_common = most_common[:MAX_VOCAB_SIZE - 4]\n\nitos = [PAD, UNK, BOS, EOS] + most_common\nstoi = {w: i for i, w in enumerate(itos)}\nvocab_size = len(itos)\nvocab_size\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:33.209291Z","iopub.execute_input":"2025-09-05T19:37:33.209582Z","iopub.status.idle":"2025-09-05T19:37:35.159141Z","shell.execute_reply.started":"2025-09-05T19:37:33.209555Z","shell.execute_reply":"2025-09-05T19:37:35.158567Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"50000"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def encode(tokens, add_bos_eos=True, max_len=MAX_LEN):\n    ids = [stoi.get(t, stoi[UNK]) for t in tokens]\n    if add_bos_eos:\n        ids = [stoi[BOS]] + ids + [stoi[EOS]]\n    if len(ids) < max_len:\n        ids += [stoi[PAD]] * (max_len - len(ids))\n    else:\n        ids = ids[:max_len]\n    return ids\n\ndf[\"ids\"] = df[\"clean\"].map(lambda s: encode(tokenize(s)))\n\nX = np.stack(df[\"ids\"].values)\ny = df[\"label\"].values\n\nX_temp, X_test, y_temp, y_test = train_test_split(\n    X, y, test_size=TEST_SPLIT, random_state=SEED, stratify=y\n)\nval_size = VAL_SPLIT / (1 - TEST_SPLIT)\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, test_size=val_size, random_state=SEED, stratify=y_temp\n)\n\nlen(X_train), len(X_val), len(X_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:40.459105Z","iopub.execute_input":"2025-09-05T19:37:40.459924Z","iopub.status.idle":"2025-09-05T19:37:44.168118Z","shell.execute_reply.started":"2025-09-05T19:37:40.459898Z","shell.execute_reply":"2025-09-05T19:37:44.167248Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"(40000, 5000, 5000)"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"class IMDBDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.tensor(X, dtype=torch.long)\n        self.y = torch.tensor(y, dtype=torch.float32)\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n\ntrain_dl = DataLoader(IMDBDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\nval_dl   = DataLoader(IMDBDataset(X_val,   y_val),   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\ntest_dl  = DataLoader(IMDBDataset(X_test,  y_test),  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:44.169517Z","iopub.execute_input":"2025-09-05T19:37:44.169817Z","iopub.status.idle":"2025-09-05T19:37:44.243890Z","shell.execute_reply.started":"2025-09-05T19:37:44.169797Z","shell.execute_reply":"2025-09-05T19:37:44.242987Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def load_glove(path, embed_dim=EMBED_DIM):\n    word2vec = {}\n    with open(path, \"r\", encoding=\"utf8\") as f:\n        for line in f:\n            parts = line.rstrip().split(\" \")\n            w, vec = parts[0], np.asarray(parts[1:], dtype=np.float32)\n            if vec.size == embed_dim: word2vec[w] = vec\n    return word2vec\n\ndef build_embedding_matrix(itos, glove_dict, embed_dim=EMBED_DIM):\n    mat = np.random.normal(0, 0.05, (len(itos), embed_dim)).astype(np.float32)\n    mat[0] = 0.0  # PAD row zeros\n    for i, w in enumerate(itos):\n        if w in (PAD, UNK, BOS, EOS): continue\n        if w in glove_dict: mat[i] = glove_dict[w]\n    return torch.tensor(mat)\n\nembedding_matrix = None\nif GLOVE_TXT_PATH and os.path.exists(GLOVE_TXT_PATH):\n    glove = load_glove(GLOVE_TXT_PATH, EMBED_DIM)\n    embedding_matrix = build_embedding_matrix(itos, glove, EMBED_DIM)\n    print(\"GloVe loaded; matrix:\", embedding_matrix.shape)\nelse:\n    print(\"GloVe not provided → using learned embeddings.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:47.232399Z","iopub.execute_input":"2025-09-05T19:37:47.232909Z","iopub.status.idle":"2025-09-05T19:37:47.242040Z","shell.execute_reply.started":"2025-09-05T19:37:47.232860Z","shell.execute_reply":"2025-09-05T19:37:47.241045Z"}},"outputs":[{"name":"stdout","text":"GloVe not provided → using learned embeddings.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class RNNClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, use_glove=False, embedding_matrix=None):\n        super().__init__()\n        if use_glove and embedding_matrix is not None:\n            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True)\n        self.fc  = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        _, h = self.rnn(x)            # h: (1,B,H)\n        return self.fc(h.squeeze(0)).squeeze(1)\n\nclass LSTMClassifier(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, use_glove=False, embedding_matrix=None):\n        super().__init__()\n        if use_glove and embedding_matrix is not None:\n            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False, padding_idx=0)\n        else:\n            self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n        self.fc   = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        _, (h, _) = self.lstm(x)      # h: (1,B,H)\n        return self.fc(h.squeeze(0)).squeeze(1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:49.892702Z","iopub.execute_input":"2025-09-05T19:37:49.893265Z","iopub.status.idle":"2025-09-05T19:37:49.900598Z","shell.execute_reply.started":"2025-09-05T19:37:49.893239Z","shell.execute_reply":"2025-09-05T19:37:49.899677Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def run_epoch(model, loader, criterion, optimizer=None):\n    train = optimizer is not None\n    model.train(train)\n    losses, preds, trues = [], [], []\n    for xb, yb in loader:\n        xb, yb = xb.to(device), yb.to(device)\n        with torch.set_grad_enabled(train):\n            logits = model(xb)\n            loss = criterion(logits, yb)\n            if train:\n                optimizer.zero_grad(set_to_none=True)\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n                optimizer.step()\n        losses.append(loss.item())\n        preds.extend((torch.sigmoid(logits) > 0.5).long().cpu().numpy())\n        trues.extend(yb.long().cpu().numpy())\n    return float(np.mean(losses)), accuracy_score(trues, preds), f1_score(trues, preds)\n\ndef train_model(model, train_dl, val_dl, epochs=EPOCHS, lr=LR):\n    model = model.to(device)\n    crit = nn.BCEWithLogitsLoss()\n    opt  = torch.optim.Adam(model.parameters(), lr=lr)\n\n    best_f1, best_state = -1, None\n    for ep in range(1, epochs+1):\n        tr_l, tr_a, tr_f = run_epoch(model, train_dl, crit, opt)\n        va_l, va_a, va_f = run_epoch(model, val_dl,   crit, None)\n        print(f\"Epoch {ep:02d} | train loss={tr_l:.4f} acc={tr_a:.4f} f1={tr_f:.4f} || \"\n              f\"val loss={va_l:.4f} acc={va_a:.4f} f1={va_f:.4f}\")\n        if va_f > best_f1:\n            best_f1, best_state = va_f, {k: v.cpu() for k, v in model.state_dict().items()}\n    if best_state:\n        model.load_state_dict(best_state)\n        model = model.to(device)\n    return model\n\ndef test_model(model, test_dl):\n    crit = nn.BCEWithLogitsLoss()\n    te_l, te_a, te_f = run_epoch(model, test_dl, crit, None)\n    print(f\"TEST  | loss={te_l:.4f} acc={te_a:.4f} f1={te_f:.4f}\")\n    return {\"loss\": te_l, \"acc\": te_a, \"f1\": te_f}\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:54.099379Z","iopub.execute_input":"2025-09-05T19:37:54.099969Z","iopub.status.idle":"2025-09-05T19:37:54.109770Z","shell.execute_reply.started":"2025-09-05T19:37:54.099942Z","shell.execute_reply":"2025-09-05T19:37:54.108733Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def run_experiment(model_cls, use_glove, name):\n    model = model_cls(\n        vocab_size=vocab_size,\n        embed_dim=EMBED_DIM,\n        hidden_dim=HIDDEN_DIM,\n        use_glove=use_glove and (embedding_matrix is not None),\n        embedding_matrix=embedding_matrix if use_glove else None\n    )\n    print(f\"\\n=== {name} ===\")\n    model = train_model(model, train_dl, val_dl, epochs=EPOCHS, lr=LR)\n    return test_model(model, test_dl)\n\nresults = {}\nresults[\"RNN+GloVe\"]     = run_experiment(RNNClassifier,  True,  \"RNN (GloVe)\")\nresults[\"LSTM+GloVe\"]    = run_experiment(LSTMClassifier, True,  \"LSTM (GloVe)\")\nresults[\"RNN+Learned\"]   = run_experiment(RNNClassifier,  False, \"RNN (Learned Embedding)\")\nresults[\"LSTM+Learned\"]  = run_experiment(LSTMClassifier, False, \"LSTM (Learned Embedding)\")\nresults\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:37:55.892020Z","iopub.execute_input":"2025-09-05T19:37:55.892291Z","iopub.status.idle":"2025-09-05T19:39:38.549038Z","shell.execute_reply.started":"2025-09-05T19:37:55.892274Z","shell.execute_reply":"2025-09-05T19:39:38.548118Z"}},"outputs":[{"name":"stdout","text":"\n=== RNN (GloVe) ===\nEpoch 01 | train loss=0.6963 acc=0.5011 f1=0.4700 || val loss=0.6976 acc=0.5014 f1=0.6549\nEpoch 02 | train loss=0.6973 acc=0.5049 f1=0.5201 || val loss=0.7008 acc=0.5002 f1=0.0008\nEpoch 03 | train loss=0.6942 acc=0.5111 f1=0.5064 || val loss=0.6997 acc=0.5020 f1=0.0518\nEpoch 04 | train loss=0.6889 acc=0.5266 f1=0.5215 || val loss=0.6938 acc=0.5092 f1=0.6321\nEpoch 05 | train loss=0.6713 acc=0.5495 f1=0.5468 || val loss=0.7105 acc=0.5068 f1=0.6562\nTEST  | loss=0.7135 acc=0.5012 f1=0.6530\n\n=== LSTM (GloVe) ===\nEpoch 01 | train loss=0.6926 acc=0.5115 f1=0.4856 || val loss=0.6930 acc=0.5244 f1=0.4467\nEpoch 02 | train loss=0.6808 acc=0.5595 f1=0.5502 || val loss=0.6858 acc=0.5492 f1=0.3178\nEpoch 03 | train loss=0.6255 acc=0.6688 f1=0.6603 || val loss=0.6308 acc=0.6794 f1=0.6498\nEpoch 04 | train loss=0.5450 acc=0.7435 f1=0.7251 || val loss=0.5741 acc=0.6768 f1=0.7308\nEpoch 05 | train loss=0.4382 acc=0.8100 f1=0.8095 || val loss=0.4621 acc=0.8028 f1=0.7916\nTEST  | loss=0.4683 acc=0.7944 f1=0.7851\n\n=== RNN (Learned Embedding) ===\nEpoch 01 | train loss=0.6963 acc=0.5034 f1=0.5113 || val loss=0.7008 acc=0.4892 f1=0.3792\nEpoch 02 | train loss=0.6984 acc=0.5007 f1=0.5005 || val loss=0.6931 acc=0.5028 f1=0.1140\nEpoch 03 | train loss=0.6965 acc=0.5019 f1=0.4971 || val loss=0.6957 acc=0.4976 f1=0.2484\nEpoch 04 | train loss=0.6960 acc=0.5133 f1=0.5047 || val loss=0.6930 acc=0.5086 f1=0.2539\nEpoch 05 | train loss=0.6909 acc=0.5222 f1=0.5201 || val loss=0.6961 acc=0.5116 f1=0.4762\nTEST  | loss=0.6962 acc=0.5098 f1=0.4799\n\n=== LSTM (Learned Embedding) ===\nEpoch 01 | train loss=0.6919 acc=0.5175 f1=0.5324 || val loss=0.6932 acc=0.5018 f1=0.1798\nEpoch 02 | train loss=0.6642 acc=0.5778 f1=0.5814 || val loss=0.5816 acc=0.7386 f1=0.7480\nEpoch 03 | train loss=0.4740 acc=0.7813 f1=0.7816 || val loss=0.3712 acc=0.8418 f1=0.8436\nEpoch 04 | train loss=0.2782 acc=0.8908 f1=0.8914 || val loss=0.3208 acc=0.8672 f1=0.8697\nEpoch 05 | train loss=0.1802 acc=0.9345 f1=0.9346 || val loss=0.3276 acc=0.8742 f1=0.8758\nTEST  | loss=0.3211 acc=0.8722 f1=0.8749\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'RNN+GloVe': {'loss': 0.7135331653341462,\n  'acc': 0.5012,\n  'f1': 0.6530328324986088},\n 'LSTM+GloVe': {'loss': 0.46826197414458554,\n  'acc': 0.7944,\n  'f1': 0.7851170568561872},\n 'RNN+Learned': {'loss': 0.6961947264550608,\n  'acc': 0.5098,\n  'f1': 0.47994907702100575},\n 'LSTM+Learned': {'loss': 0.32107965384103077,\n  'acc': 0.8722,\n  'f1': 0.8748776189543763}}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"pd.DataFrame(results).T.sort_values(\"f1\", ascending=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T19:39:50.268930Z","iopub.execute_input":"2025-09-05T19:39:50.269623Z","iopub.status.idle":"2025-09-05T19:39:50.285147Z","shell.execute_reply.started":"2025-09-05T19:39:50.269595Z","shell.execute_reply":"2025-09-05T19:39:50.284377Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                  loss     acc        f1\nLSTM+Learned  0.321080  0.8722  0.874878\nLSTM+GloVe    0.468262  0.7944  0.785117\nRNN+GloVe     0.713533  0.5012  0.653033\nRNN+Learned   0.696195  0.5098  0.479949","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loss</th>\n      <th>acc</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>LSTM+Learned</th>\n      <td>0.321080</td>\n      <td>0.8722</td>\n      <td>0.874878</td>\n    </tr>\n    <tr>\n      <th>LSTM+GloVe</th>\n      <td>0.468262</td>\n      <td>0.7944</td>\n      <td>0.785117</td>\n    </tr>\n    <tr>\n      <th>RNN+GloVe</th>\n      <td>0.713533</td>\n      <td>0.5012</td>\n      <td>0.653033</td>\n    </tr>\n    <tr>\n      <th>RNN+Learned</th>\n      <td>0.696195</td>\n      <td>0.5098</td>\n      <td>0.479949</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}